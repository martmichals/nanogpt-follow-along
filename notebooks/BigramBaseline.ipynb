{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b1b94b6-88b8-4b7b-b459-e5b880c12fdd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9c54de0-7be3-4e01-b8c7-d7ebc319573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e804868b-85cf-4bc6-bffb-4a03bb36ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de13db-8d1a-40db-b6e3-84f2813c2aaf",
   "metadata": {},
   "source": [
    "# Setting Up Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45f10d88-522a-47b9-9fda-5776c2203fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "with open('../data/input.txt', 'r', encoding='utf-8') as inf:\n",
    "    text = inf.read()\n",
    "print(f'Length of dataset in characters: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ac49788-a6e1-44d2-a2da-7dc303d83a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "# Set up the vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(f'Vocabulary size: {len(chars)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "625216af-761a-4dfc-9b09-2bcc382d1bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the encoder and decoder\n",
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[ch] for ch in s] # str -> List[int]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])   # List[int] -> str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b42de-c766-4dc2-a1c4-db136f58509b",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f8c8b2-65e3-4bfc-ab91-0f4e7c0126e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmartmichals\u001b[0m (\u001b[33mmartymcfly\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/martmichals/envs/nanogpt-follow-along/notebooks/wandb/run-20231216_133646-mq2rnkvk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/martymcfly/nanogpt/runs/mq2rnkvk' target=\"_blank\">train-bigram-baseline</a></strong> to <a href='https://wandb.ai/martymcfly/nanogpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/martymcfly/nanogpt' target=\"_blank\">https://wandb.ai/martymcfly/nanogpt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/martymcfly/nanogpt/runs/mq2rnkvk' target=\"_blank\">https://wandb.ai/martymcfly/nanogpt/runs/mq2rnkvk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(name='train-bigram-baseline', job_type='training')\n",
    "artifact = run.use_artifact('nanogpt/mini-shakespeare-tensors:latest')\n",
    "data_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d423ca8-b52d-4919-a9b2-9f4a071da8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train = torch.load(os.path.join(data_dir, 'train.pt'))\n",
    "val   = torch.load(os.path.join(data_dir, 'val.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ddb03fa-9b28-439e-9cf0-bcc5f79f63d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Context window size\n",
    "block_size = 8\n",
    "train[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234f60a-79ee-44e1-95c6-c7ac19719e44",
   "metadata": {},
   "source": [
    "Below are examples of training samples for the transformer. We arrange the training samples this way s.t. at runtime, the model is used to seeing context sizes of varying lengths, from 1 token of context all the way up until `block_size` tokens of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61307eee-427d-4dac-a2e0-8f778e2946bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target is: 47\n",
      "When input is tensor([18, 47]) the target is: 56\n",
      "When input is tensor([18, 47, 56]) the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "# Show training samples\n",
    "x = train[:block_size]\n",
    "y = train[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'When input is {context} the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e553911-c465-4eb3-a2c8-94fe4aeff3c4",
   "metadata": {},
   "source": [
    "Here we define a function to sample batches from the training or validation datasets. The returned samples are all `block_size` tokens long, with offset prediction tokens. Refer to this later in the training process, since the above implies the training process needs to somehow learn across varying input sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62cc9f26-6fbc-44ef-9dda-efb962a9dbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Output:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "-------\n",
      "Enumeration of all training samples\n",
      "When input is tensor([24]) the target is: 43\n",
      "When input is tensor([24, 43]) the target is: 58\n",
      "When input is tensor([24, 43, 58]) the target is: 5\n",
      "When input is tensor([24, 43, 58,  5]) the target is: 57\n",
      "When input is tensor([24, 43, 58,  5, 57]) the target is: 1\n",
      "When input is tensor([24, 43, 58,  5, 57,  1]) the target is: 46\n",
      "When input is tensor([24, 43, 58,  5, 57,  1, 46]) the target is: 43\n",
      "When input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target is: 39\n",
      "When input is tensor([44]) the target is: 53\n",
      "When input is tensor([44, 53]) the target is: 56\n",
      "When input is tensor([44, 53, 56]) the target is: 1\n",
      "When input is tensor([44, 53, 56,  1]) the target is: 58\n",
      "When input is tensor([44, 53, 56,  1, 58]) the target is: 46\n",
      "When input is tensor([44, 53, 56,  1, 58, 46]) the target is: 39\n",
      "When input is tensor([44, 53, 56,  1, 58, 46, 39]) the target is: 58\n",
      "When input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target is: 1\n",
      "When input is tensor([52]) the target is: 58\n",
      "When input is tensor([52, 58]) the target is: 1\n",
      "When input is tensor([52, 58,  1]) the target is: 58\n",
      "When input is tensor([52, 58,  1, 58]) the target is: 46\n",
      "When input is tensor([52, 58,  1, 58, 46]) the target is: 39\n",
      "When input is tensor([52, 58,  1, 58, 46, 39]) the target is: 58\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58]) the target is: 1\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target is: 46\n",
      "When input is tensor([25]) the target is: 17\n",
      "When input is tensor([25, 17]) the target is: 27\n",
      "When input is tensor([25, 17, 27]) the target is: 10\n",
      "When input is tensor([25, 17, 27, 10]) the target is: 0\n",
      "When input is tensor([25, 17, 27, 10,  0]) the target is: 21\n",
      "When input is tensor([25, 17, 27, 10,  0, 21]) the target is: 1\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1]) the target is: 54\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target is: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split: str):\n",
    "    # Generate a batch of inputs x and targets y\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Show a batch sample\n",
    "xb, yb = get_batch('train')\n",
    "print('Input:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('Output:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('-------\\nEnumeration of all training samples')\n",
    "\n",
    "# Show all the training samples, xb is 4 x 8, which means we have 32 independent training examples, enumerated below\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f'When input is {context} the target is: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4789a2fa-67a2-4747-a351-8cfe745544f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "# Show sample\n",
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a366d88-9ec9-4efd-8e57-a1ce27136f7d",
   "metadata": {},
   "source": [
    "What are logits?\n",
    "\r\n",
    "Logits are the raw, unnormalized output of a neural network's final layer before it is passed through a softmax activation function. In the context of machine learning and deep learning, logits typically refer to the raw scores or values produced by the last layer of a neural network, just before applying the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18b1d726-7ae0-46d5-9f8d-44bf784b4604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C), since idx: (B, T) and (C), i.e. vocab_size logits per token\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # cross-entropy loss expects (N, C) where N is no. samples and c is no. classes as input\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes  (B, C)\n",
    "            # apply softmax for probability distribution\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled idx to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "vocab_size = 65 # from the data setup notebook\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss) # loss is expected to be roughly -ln(1/65), use the cross entropy loss fxn to estimate\n",
    "\n",
    "# test generation\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d151eebd-59b0-4475-b7d2-c541f11a59ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3bef12ec-fe20-48ac-886b-f7d110dc3775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.375595808029175\n"
     ]
    }
   ],
   "source": [
    "# Train the optimizer\n",
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f0b5fba-f93c-4a40-821f-9db35b0617c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ABRGomyrothas ie lf IUCl t y d,\n",
      "VOLUS:\n",
      "Lo t! or yowou is weser ppt, ps f oucilbo, wofe: caisee rin ICHincoutos Pugibosser theyon, n, f thid qure aimeme, shoro, he tr elonchyin s f ond D ad todin come hther he.\n",
      "Whtcigo athepe mo lane t?\n",
      "\n",
      "\n",
      "\n",
      "AUThit a hifat ngrustt iunedines al d yofre ak wive as ty t:\n",
      "A thathefango bllyesmeoule by ma\n",
      "CARe th d.\n",
      "Whate,\n",
      "'junveres poe ty way\n",
      "Th ph gon Ithe, t mar, ha, t nth ll hast ghatarld mersife mast'd fe heanksofthatheal! f dsalisthe n s uno inghe thorerdichailerd\n"
     ]
    }
   ],
   "source": [
    "# test generation\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "nanogpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
